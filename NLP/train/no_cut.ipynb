{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"./savenocut/data_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data[data[\"label\"] == 2].index).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 done\n",
      "1 done\n",
      "2 done\n",
      "3 done\n",
      "4 done\n",
      "5 done\n",
      "6 done\n",
      "7 done\n",
      "8 done\n",
      "9 done\n",
      "10 done\n",
      "11 done\n",
      "12 done\n",
      "13 done\n",
      "14 done\n",
      "15 done\n",
      "16 done\n",
      "17 done\n",
      "18 done\n",
      "19 done\n",
      "20 done\n",
      "21 done\n",
      "22 done\n",
      "23 done\n",
      "24 done\n",
      "25 done\n",
      "26 done\n",
      "27 done\n",
      "28 done\n",
      "29 done\n",
      "30 done\n",
      "31 done\n",
      "32 done\n",
      "33 done\n",
      "34 done\n",
      "35 done\n",
      "36 done\n",
      "37 done\n",
      "38 done\n",
      "39 done\n",
      "40 done\n",
      "41 done\n",
      "42 done\n",
      "43 done\n",
      "44 done\n",
      "45 done\n",
      "46 done\n",
      "47 done\n",
      "48 done\n",
      "49 done\n",
      "50 done\n",
      "51 done\n",
      "52 done\n",
      "53 done\n",
      "54 done\n",
      "55 done\n",
      "56 done\n",
      "57 done\n",
      "58 done\n",
      "59 done\n",
      "60 done\n",
      "61 done\n",
      "62 done\n",
      "63 done\n",
      "64 done\n",
      "65 done\n",
      "66 done\n",
      "67 done\n",
      "68 done\n",
      "69 done\n",
      "70 done\n",
      "71 done\n",
      "72 done\n",
      "73 done\n",
      "74 done\n",
      "75 done\n",
      "76 done\n",
      "77 done\n",
      "78 done\n",
      "79 done\n",
      "80 done\n",
      "81 done\n",
      "82 done\n",
      "83 done\n",
      "84 done\n",
      "85 done\n",
      "86 done\n",
      "87 done\n",
      "88 done\n",
      "89 done\n",
      "90 done\n",
      "91 done\n",
      "92 done\n",
      "93 done\n",
      "94 done\n",
      "95 done\n",
      "96 done\n",
      "97 done\n",
      "98 done\n",
      "99 done\n",
      "100 done\n",
      "101 done\n",
      "102 done\n",
      "103 done\n",
      "104 done\n",
      "105 done\n",
      "106 done\n",
      "107 done\n",
      "108 done\n",
      "109 done\n",
      "110 done\n",
      "111 done\n",
      "112 done\n",
      "113 done\n",
      "114 done\n",
      "115 done\n",
      "116 done\n",
      "117 done\n",
      "118 done\n",
      "119 done\n",
      "120 done\n",
      "121 done\n",
      "122 done\n",
      "123 done\n",
      "124 done\n",
      "125 done\n",
      "126 done\n",
      "127 done\n",
      "128 done\n",
      "129 done\n",
      "130 done\n",
      "131 done\n",
      "132 done\n",
      "133 done\n",
      "134 done\n",
      "135 done\n",
      "136 done\n",
      "137 done\n",
      "138 done\n",
      "139 done\n",
      "140 done\n",
      "141 done\n",
      "142 done\n",
      "143 done\n",
      "144 done\n",
      "145 done\n",
      "146 done\n",
      "147 done\n",
      "148 done\n",
      "149 done\n",
      "150 done\n",
      "151 done\n",
      "152 done\n",
      "153 done\n",
      "154 done\n",
      "155 done\n",
      "156 done\n",
      "157 done\n",
      "158 done\n",
      "159 done\n",
      "160 done\n",
      "161 done\n",
      "162 done\n",
      "163 done\n",
      "164 done\n",
      "165 done\n",
      "166 done\n",
      "167 done\n",
      "168 done\n",
      "169 done\n",
      "170 done\n",
      "171 done\n",
      "172 done\n",
      "173 done\n",
      "174 done\n",
      "175 done\n",
      "176 done\n",
      "177 done\n",
      "178 done\n",
      "179 done\n",
      "180 done\n",
      "181 done\n",
      "182 done\n",
      "183 done\n",
      "184 done\n",
      "185 done\n",
      "186 done\n",
      "187 done\n",
      "188 done\n",
      "189 done\n",
      "190 done\n",
      "191 done\n",
      "192 done\n",
      "193 done\n",
      "194 done\n",
      "195 done\n",
      "196 done\n",
      "197 done\n",
      "198 done\n",
      "199 done\n",
      "200 done\n",
      "201 done\n",
      "202 done\n",
      "203 done\n",
      "204 done\n",
      "205 done\n",
      "206 done\n",
      "207 done\n",
      "208 done\n",
      "209 done\n",
      "210 done\n",
      "211 done\n",
      "212 done\n",
      "213 done\n",
      "214 done\n",
      "215 done\n",
      "216 done\n",
      "217 done\n",
      "218 done\n",
      "219 done\n",
      "220 done\n",
      "221 done\n",
      "222 done\n",
      "223 done\n",
      "224 done\n",
      "225 done\n",
      "226 done\n",
      "227 done\n",
      "228 done\n",
      "229 done\n",
      "230 done\n",
      "231 done\n",
      "232 done\n",
      "233 done\n",
      "234 done\n",
      "235 done\n",
      "236 done\n",
      "237 done\n",
      "238 done\n",
      "239 done\n",
      "240 done\n",
      "241 done\n",
      "242 done\n",
      "243 done\n",
      "244 done\n",
      "245 done\n",
      "246 done\n",
      "247 done\n",
      "248 done\n",
      "249 done\n",
      "250 done\n",
      "251 done\n",
      "252 done\n",
      "253 done\n",
      "254 done\n",
      "255 done\n",
      "256 done\n",
      "257 done\n",
      "258 done\n",
      "259 done\n",
      "260 done\n",
      "261 done\n",
      "262 done\n",
      "263 done\n",
      "264 done\n",
      "265 done\n",
      "266 done\n",
      "267 done\n",
      "268 done\n",
      "269 done\n",
      "270 done\n",
      "271 done\n",
      "272 done\n",
      "273 done\n",
      "274 done\n",
      "275 done\n",
      "276 done\n",
      "277 done\n",
      "278 done\n",
      "279 done\n",
      "280 done\n",
      "281 done\n",
      "282 done\n",
      "283 done\n",
      "284 done\n",
      "285 done\n",
      "286 done\n",
      "287 done\n",
      "288 done\n",
      "289 done\n",
      "290 done\n",
      "291 done\n",
      "292 done\n",
      "293 done\n",
      "294 done\n",
      "295 done\n",
      "296 done\n",
      "297 done\n",
      "298 done\n",
      "299 done\n",
      "300 done\n",
      "301 done\n",
      "302 done\n",
      "303 done\n",
      "304 done\n",
      "305 done\n",
      "306 done\n",
      "307 done\n",
      "308 done\n",
      "309 done\n",
      "310 done\n",
      "311 done\n",
      "312 done\n",
      "313 done\n",
      "314 done\n",
      "315 done\n",
      "316 done\n",
      "317 done\n",
      "318 done\n",
      "319 done\n",
      "320 done\n",
      "321 done\n",
      "322 done\n",
      "323 done\n",
      "324 done\n",
      "325 done\n",
      "326 done\n",
      "327 done\n",
      "328 done\n",
      "329 done\n",
      "330 done\n",
      "331 done\n",
      "332 done\n",
      "333 done\n",
      "334 done\n",
      "335 done\n",
      "336 done\n",
      "337 done\n",
      "338 done\n",
      "339 done\n",
      "340 done\n",
      "341 done\n",
      "342 done\n",
      "343 done\n",
      "344 done\n",
      "345 done\n",
      "346 done\n",
      "347 done\n",
      "348 done\n",
      "349 done\n",
      "350 done\n",
      "351 done\n",
      "352 done\n",
      "353 done\n",
      "354 done\n",
      "355 done\n",
      "356 done\n",
      "357 done\n",
      "358 done\n",
      "359 done\n",
      "360 done\n",
      "361 done\n",
      "362 done\n",
      "363 done\n",
      "364 done\n",
      "365 done\n",
      "366 done\n",
      "367 done\n",
      "368 done\n",
      "369 done\n",
      "370 done\n",
      "371 done\n",
      "372 done\n",
      "373 done\n",
      "374 done\n",
      "375 done\n",
      "376 done\n",
      "377 done\n",
      "378 done\n",
      "379 done\n",
      "380 done\n",
      "381 done\n",
      "382 done\n",
      "383 done\n",
      "384 done\n",
      "385 done\n",
      "386 done\n",
      "387 done\n",
      "388 done\n",
      "389 done\n",
      "390 done\n",
      "391 done\n",
      "392 done\n",
      "393 done\n",
      "394 done\n",
      "395 done\n",
      "396 done\n",
      "397 done\n",
      "398 done\n",
      "399 done\n",
      "400 done\n",
      "401 done\n",
      "402 done\n",
      "403 done\n",
      "404 done\n",
      "405 done\n",
      "406 done\n",
      "407 done\n",
      "408 done\n",
      "409 done\n",
      "410 done\n",
      "411 done\n",
      "412 done\n",
      "413 done\n",
      "414 done\n",
      "415 done\n",
      "416 done\n",
      "417 done\n",
      "418 done\n",
      "419 done\n",
      "420 done\n",
      "421 done\n",
      "422 done\n",
      "423 done\n",
      "424 done\n",
      "425 done\n",
      "426 done\n",
      "427 done\n",
      "428 done\n",
      "429 done\n",
      "430 done\n",
      "431 done\n",
      "432 done\n",
      "433 done\n",
      "434 done\n",
      "435 done\n",
      "436 done\n",
      "437 done\n",
      "438 done\n",
      "439 done\n",
      "440 done\n",
      "441 done\n",
      "442 done\n",
      "443 done\n",
      "444 done\n",
      "445 done\n",
      "446 done\n",
      "447 done\n",
      "448 done\n",
      "449 done\n",
      "450 done\n",
      "451 done\n",
      "452 done\n",
      "453 done\n",
      "454 done\n",
      "455 done\n",
      "456 done\n",
      "457 done\n",
      "458 done\n",
      "459 done\n",
      "460 done\n",
      "461 done\n",
      "462 done\n",
      "463 done\n",
      "464 done\n",
      "465 done\n",
      "466 done\n",
      "467 done\n",
      "468 done\n",
      "469 done\n",
      "470 done\n",
      "471 done\n",
      "472 done\n",
      "473 done\n",
      "474 done\n",
      "475 done\n",
      "476 done\n",
      "477 done\n",
      "478 done\n",
      "479 done\n",
      "480 done\n",
      "481 done\n",
      "482 done\n",
      "483 done\n",
      "484 done\n",
      "485 done\n",
      "486 done\n",
      "487 done\n",
      "488 done\n",
      "489 done\n",
      "490 done\n",
      "491 done\n",
      "492 done\n",
      "493 done\n",
      "494 done\n",
      "495 done\n",
      "496 done\n",
      "497 done\n",
      "498 done\n",
      "499 done\n",
      "500 done\n",
      "501 done\n",
      "502 done\n",
      "503 done\n",
      "504 done\n",
      "505 done\n",
      "506 done\n",
      "507 done\n",
      "508 done\n",
      "509 done\n",
      "510 done\n",
      "511 done\n",
      "512 done\n",
      "513 done\n",
      "514 done\n",
      "515 done\n",
      "516 done\n",
      "517 done\n",
      "518 done\n",
      "519 done\n",
      "520 done\n",
      "521 done\n",
      "522 done\n",
      "523 done\n",
      "524 done\n",
      "525 done\n",
      "526 done\n",
      "527 done\n",
      "528 done\n",
      "529 done\n",
      "530 done\n",
      "531 done\n",
      "532 done\n",
      "533 done\n",
      "534 done\n",
      "535 done\n",
      "536 done\n",
      "537 done\n",
      "538 done\n",
      "539 done\n",
      "540 done\n",
      "541 done\n",
      "542 done\n",
      "543 done\n",
      "544 done\n",
      "545 done\n",
      "546 done\n",
      "547 done\n",
      "548 done\n",
      "549 done\n",
      "550 done\n",
      "551 done\n",
      "552 done\n",
      "553 done\n",
      "554 done\n",
      "555 done\n",
      "556 done\n",
      "557 done\n",
      "558 done\n",
      "559 done\n",
      "560 done\n",
      "561 done\n",
      "562 done\n",
      "563 done\n",
      "564 done\n",
      "565 done\n",
      "566 done\n",
      "567 done\n",
      "568 done\n",
      "569 done\n",
      "570 done\n",
      "571 done\n",
      "572 done\n",
      "573 done\n",
      "574 done\n",
      "575 done\n",
      "576 done\n",
      "577 done\n",
      "578 done\n",
      "579 done\n",
      "580 done\n",
      "581 done\n",
      "582 done\n",
      "583 done\n",
      "584 done\n",
      "585 done\n",
      "586 done\n",
      "587 done\n",
      "588 done\n",
      "589 done\n",
      "590 done\n",
      "591 done\n",
      "592 done\n",
      "593 done\n",
      "594 done\n",
      "595 done\n",
      "596 done\n",
      "597 done\n",
      "598 done\n",
      "599 done\n",
      "600 done\n",
      "601 done\n",
      "602 done\n",
      "603 done\n",
      "604 done\n",
      "605 done\n",
      "606 done\n",
      "607 done\n",
      "608 done\n",
      "609 done\n",
      "610 done\n",
      "611 done\n",
      "612 done\n",
      "613 done\n",
      "614 done\n",
      "615 done\n",
      "616 done\n",
      "617 done\n",
      "618 done\n",
      "619 done\n",
      "620 done\n",
      "621 done\n",
      "622 done\n",
      "623 done\n",
      "624 done\n",
      "625 done\n",
      "626 done\n",
      "627 done\n",
      "628 done\n",
      "629 done\n",
      "630 done\n",
      "631 done\n",
      "632 done\n",
      "633 done\n",
      "634 done\n",
      "635 done\n",
      "636 done\n",
      "637 done\n",
      "638 done\n",
      "639 done\n",
      "640 done\n",
      "641 done\n",
      "642 done\n",
      "643 done\n",
      "644 done\n",
      "645 done\n",
      "646 done\n",
      "647 done\n",
      "648 done\n",
      "649 done\n",
      "650 done\n",
      "651 done\n",
      "652 done\n",
      "653 done\n",
      "654 done\n",
      "655 done\n",
      "656 done\n",
      "657 done\n",
      "658 done\n",
      "659 done\n",
      "660 done\n",
      "661 done\n",
      "662 done\n",
      "663 done\n",
      "664 done\n",
      "665 done\n",
      "666 done\n",
      "667 done\n",
      "668 done\n",
      "669 done\n",
      "670 done\n",
      "671 done\n",
      "672 done\n",
      "673 done\n",
      "674 done\n",
      "675 done\n",
      "676 done\n",
      "677 done\n",
      "678 done\n",
      "679 done\n",
      "680 done\n",
      "681 done\n",
      "682 done\n",
      "683 done\n",
      "684 done\n",
      "685 done\n",
      "686 done\n",
      "687 done\n",
      "688 done\n",
      "689 done\n",
      "690 done\n",
      "691 done\n",
      "692 done\n",
      "693 done\n",
      "694 done\n",
      "695 done\n"
     ]
    }
   ],
   "source": [
    "#段詞\n",
    "from inlp.convert import chinese\n",
    "from ckip import CkipSegmenter\n",
    "import json\n",
    "\n",
    "train_recovery = []\n",
    "num = 0\n",
    "\n",
    "for i in train[\"recovery\"]:\n",
    "    i = chinese.s2t(i) #簡體轉繁體\n",
    "    recovery = CkipSegmenter().seg(i).tok\n",
    "    train_recovery.append(recovery)\n",
    "    print(num, \"done\")\n",
    "    num = num + 1\n",
    "    \n",
    "# data = data.drop(data.index[error]).reset_index(drop = True)\n",
    "# data.to_csv('./savenocut/data_clean.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "    #保存list\n",
    "c_list = json.dumps(train_recovery)\n",
    "a = open(r\"./savenocut/list_save.txt\", \"w\",encoding='UTF-8')\n",
    "a.write(c_list)\n",
    "a.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#產生一個token_list的Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(train_recovery, window=5, size=500, sg=1, iter=50, min_count=1)\n",
    "#儲存他\n",
    "model.save(\"./savenocut/model_Word2Vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "embedding_matrix = np.zeros((len(model.wv.vocab.items()) + 1, model.vector_size))\n",
    "word2idx = {}\n",
    "PADDING_LENGTH = 500\n",
    "vocab_list = [(word, model.wv[word]) for word, _ in model.wv.vocab.items()]\n",
    "\n",
    "for i, vocab in enumerate(vocab_list):\n",
    "    word, vec = vocab\n",
    "    embedding_matrix[i + 1] = vec\n",
    "    word2idx[word] = i + 1\n",
    "\n",
    "embedding_layer = Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                            output_dim=embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)\n",
    "\n",
    "#### mapping to index\n",
    "def text_to_index(corpus):\n",
    "    new_corpus = []\n",
    "    for doc in corpus:\n",
    "        new_doc = []\n",
    "        for word in doc:\n",
    "            try:\n",
    "                new_doc.append(word2idx[word])\n",
    "            except:\n",
    "                new_doc.append(0)\n",
    "        new_corpus.append(new_doc)\n",
    "    return np.array(new_corpus)\n",
    "\n",
    "def new_model():\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-0467c45ce11a>:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(new_corpus)\n"
     ]
    }
   ],
   "source": [
    "#### 加上label\n",
    "X = text_to_index(train_recovery)\n",
    "X = pad_sequences(X, maxlen=PADDING_LENGTH)\n",
    "\n",
    "Y = to_categorical(train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 76s 11s/step - loss: 0.7042 - accuracy: 0.4762\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 93s 13s/step - loss: 0.6461 - accuracy: 0.6598\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 100s 14s/step - loss: 0.6167 - accuracy: 0.6604\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 107s 15s/step - loss: 0.5953 - accuracy: 0.6873\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 122s 18s/step - loss: 0.5718 - accuracy: 0.6983\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 132s 19s/step - loss: 0.5213 - accuracy: 0.7249\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 145s 21s/step - loss: 0.5177 - accuracy: 0.7219\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 148s 21s/step - loss: 0.5087 - accuracy: 0.7597\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 152s 21s/step - loss: 0.4740 - accuracy: 0.7614\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 153s 22s/step - loss: 0.4327 - accuracy: 0.8186\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 157s 23s/step - loss: 0.3998 - accuracy: 0.8317\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 165s 23s/step - loss: 0.3467 - accuracy: 0.8541\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 168s 24s/step - loss: 0.3342 - accuracy: 0.8607\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 185s 26s/step - loss: 0.2769 - accuracy: 0.8922\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 172s 24s/step - loss: 0.2507 - accuracy: 0.8989\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 190s 27s/step - loss: 0.2174 - accuracy: 0.9244\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 201s 29s/step - loss: 0.1428 - accuracy: 0.9465\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 210s 29s/step - loss: 0.1319 - accuracy: 0.9689\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 201s 29s/step - loss: 0.1152 - accuracy: 0.9625\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 198s 28s/step - loss: 0.0972 - accuracy: 0.9631\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 191s 27s/step - loss: 0.0781 - accuracy: 0.9714\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 198s 29s/step - loss: 0.0633 - accuracy: 0.9799\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 219s 32s/step - loss: 0.0593 - accuracy: 0.9878\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 233s 33s/step - loss: 0.0574 - accuracy: 0.9836\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 228s 33s/step - loss: 0.0695 - accuracy: 0.9795\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 235s 34s/step - loss: 0.0488 - accuracy: 0.9855\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 235s 34s/step - loss: 0.0537 - accuracy: 0.9861\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 239s 34s/step - loss: 0.0494 - accuracy: 0.9884\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 241s 34s/step - loss: 0.0450 - accuracy: 0.9901\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 240s 34s/step - loss: 0.0517 - accuracy: 0.9814\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 250s 36s/step - loss: 0.0282 - accuracy: 0.9952\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 246s 35s/step - loss: 0.0262 - accuracy: 0.9941\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 236s 34s/step - loss: 0.0158 - accuracy: 0.9970\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 212s 30s/step - loss: 0.0276 - accuracy: 0.9897\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 227s 32s/step - loss: 0.0198 - accuracy: 0.9944\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 229s 33s/step - loss: 0.0202 - accuracy: 0.9948\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 224s 32s/step - loss: 0.0124 - accuracy: 0.9996\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 229s 32s/step - loss: 0.0220 - accuracy: 0.9922\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 239s 35s/step - loss: 0.0194 - accuracy: 0.9942\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 235s 33s/step - loss: 0.0132 - accuracy: 0.9968\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 193s 27s/step - loss: 0.0129 - accuracy: 0.9959\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 188s 27s/step - loss: 0.0159 - accuracy: 0.9924\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 182s 26s/step - loss: 0.0094 - accuracy: 0.9987\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 199s 29s/step - loss: 0.0271 - accuracy: 0.9918\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 190s 27s/step - loss: 0.0187 - accuracy: 0.9962\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 189s 27s/step - loss: 0.0260 - accuracy: 0.9959\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 192s 28s/step - loss: 0.0168 - accuracy: 0.9963\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 183s 26s/step - loss: 0.0179 - accuracy: 0.9973\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 190s 27s/step - loss: 0.0172 - accuracy: 0.9958\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 180s 26s/step - loss: 0.0167 - accuracy: 0.9926\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 191s 27s/step - loss: 0.0173 - accuracy: 0.9967\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 191s 27s/step - loss: 0.0147 - accuracy: 0.9942\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 186s 27s/step - loss: 0.0361 - accuracy: 0.9886\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 186s 26s/step - loss: 0.0506 - accuracy: 0.9868\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 189s 27s/step - loss: 0.0300 - accuracy: 0.9925\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 202s 29s/step - loss: 0.0259 - accuracy: 0.9867\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 187s 27s/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 196s 28s/step - loss: 0.0128 - accuracy: 0.9989\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 195s 27s/step - loss: 0.0210 - accuracy: 0.9907\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 193s 28s/step - loss: 0.0114 - accuracy: 0.9954\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 193s 28s/step - loss: 0.0125 - accuracy: 0.9936\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 192s 27s/step - loss: 0.0107 - accuracy: 0.9940\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 194s 28s/step - loss: 0.0067 - accuracy: 0.9996\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 198s 29s/step - loss: 0.0110 - accuracy: 0.9975\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 202s 29s/step - loss: 0.0073 - accuracy: 0.9978\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 207s 30s/step - loss: 0.0152 - accuracy: 0.9944\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 198s 28s/step - loss: 0.0093 - accuracy: 0.9989\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 191s 27s/step - loss: 0.0050 - accuracy: 0.9994\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 193s 28s/step - loss: 0.0065 - accuracy: 0.9989\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 197s 28s/step - loss: 0.0097 - accuracy: 0.9960\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 198s 28s/step - loss: 0.0102 - accuracy: 0.9962\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 191s 27s/step - loss: 0.0066 - accuracy: 0.9978\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 209s 30s/step - loss: 0.0126 - accuracy: 0.9938\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 206s 30s/step - loss: 0.0067 - accuracy: 0.9986\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 204s 29s/step - loss: 0.0085 - accuracy: 0.9979\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 200s 29s/step - loss: 0.0199 - accuracy: 0.9941\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 204s 29s/step - loss: 0.0197 - accuracy: 0.9904\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 198s 28s/step - loss: 0.0072 - accuracy: 0.9996\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 216s 31s/step - loss: 0.0059 - accuracy: 0.9989\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 210s 30s/step - loss: 0.0060 - accuracy: 0.9985\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 213s 31s/step - loss: 0.0043 - accuracy: 0.9986\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 212s 30s/step - loss: 0.0097 - accuracy: 0.9955\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 215s 30s/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 206s 30s/step - loss: 0.0070 - accuracy: 0.9978\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 210s 30s/step - loss: 0.0043 - accuracy: 0.9989\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 210s 30s/step - loss: 0.0041 - accuracy: 0.9996\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 214s 31s/step - loss: 0.0072 - accuracy: 0.9973\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 215s 31s/step - loss: 0.0070 - accuracy: 0.9966\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 217s 31s/step - loss: 0.0036 - accuracy: 0.9978\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 218s 31s/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 215s 31s/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 219s 31s/step - loss: 0.0045 - accuracy: 0.9966\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 219s 31s/step - loss: 0.0044 - accuracy: 0.9989\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 207s 30s/step - loss: 0.0048 - accuracy: 0.9985\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 207s 29s/step - loss: 0.0052 - accuracy: 0.9976\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 197s 28s/step - loss: 0.0243 - accuracy: 0.9915\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 200s 29s/step - loss: 0.0438 - accuracy: 0.9859\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 205s 29s/step - loss: 0.0236 - accuracy: 0.9933\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 203s 29s/step - loss: 0.0143 - accuracy: 0.9970\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 200s 28s/step - loss: 0.0151 - accuracy: 0.9953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x222b969fc10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#開始跑\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Activation, LSTM\n",
    "\n",
    "lstm = new_model()\n",
    "lstm.fit(x=X, y=Y, batch_size=100, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Activation, LSTM\n",
    "\n",
    "#儲存他\n",
    "# save model\n",
    "lstm.save_weights('./savenocut/lstm.h5')\n",
    "model_json = lstm.to_json()\n",
    "with open('./savenocut/lstm.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 未來在使用他"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Activation, LSTM\n",
    "import tensorflow as tf\n",
    "# load model\n",
    "json_file = open('./savenocut/lstm.json', 'r')\n",
    "lstm_loaded_model_json = json_file.read()\n",
    "lstm = tf.keras.models.model_from_json(lstm_loaded_model_json)\n",
    "lstm.load_weights('./savenocut/lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = test[\"recovery\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-0467c45ce11a>:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(new_corpus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.46613549e-02 9.85338688e-01]\n",
      " [3.32120389e-01 6.67879641e-01]\n",
      " [6.28577603e-04 9.99371469e-01]\n",
      " [7.42229223e-02 9.25777078e-01]\n",
      " [5.14579892e-01 4.85420167e-01]\n",
      " [7.84674048e-01 2.15325922e-01]\n",
      " [2.03694761e-01 7.96305299e-01]\n",
      " [8.55899677e-02 9.14409995e-01]\n",
      " [7.70310283e-01 2.29689747e-01]\n",
      " [5.33438213e-02 9.46656168e-01]\n",
      " [9.99988317e-01 1.17127238e-05]\n",
      " [6.69539198e-02 9.33046103e-01]\n",
      " [9.94918525e-01 5.08141285e-03]\n",
      " [2.51891047e-01 7.48108983e-01]\n",
      " [4.94967587e-02 9.50503170e-01]\n",
      " [1.59611091e-01 8.40388894e-01]\n",
      " [1.91904232e-01 8.08095813e-01]\n",
      " [9.99970794e-01 2.91808137e-05]\n",
      " [9.99916196e-01 8.37758926e-05]\n",
      " [9.74314570e-01 2.56853905e-02]\n",
      " [4.98276234e-01 5.01723766e-01]\n",
      " [6.26564384e-01 3.73435676e-01]\n",
      " [9.04566705e-01 9.54332799e-02]\n",
      " [9.97248709e-01 2.75132083e-03]\n",
      " [5.69686651e-01 4.30313349e-01]\n",
      " [7.82165094e-04 9.99217868e-01]\n",
      " [9.99890804e-01 1.09127468e-04]\n",
      " [9.97657418e-01 2.34259525e-03]\n",
      " [2.64433939e-02 9.73556638e-01]\n",
      " [2.56239716e-02 9.74376023e-01]\n",
      " [2.72673428e-01 7.27326572e-01]\n",
      " [9.97360885e-01 2.63908622e-03]\n",
      " [8.88823706e-05 9.99911070e-01]\n",
      " [9.99958158e-01 4.18363888e-05]\n",
      " [9.93887126e-01 6.11284515e-03]\n",
      " [9.59385395e-01 4.06146348e-02]\n",
      " [9.21292007e-01 7.87079856e-02]\n",
      " [9.99246478e-01 7.53538101e-04]\n",
      " [7.93091178e-01 2.06908852e-01]\n",
      " [9.81215388e-02 9.01878476e-01]\n",
      " [9.25892889e-01 7.41071105e-02]\n",
      " [4.50517207e-01 5.49482763e-01]\n",
      " [9.99977946e-01 2.20263628e-05]\n",
      " [4.39368971e-02 9.56063151e-01]\n",
      " [9.98973012e-01 1.02700724e-03]\n",
      " [7.39766806e-02 9.26023304e-01]\n",
      " [4.03875440e-01 5.96124530e-01]\n",
      " [1.39254183e-01 8.60745788e-01]\n",
      " [9.82009053e-01 1.79909263e-02]\n",
      " [9.99821484e-01 1.78519491e-04]\n",
      " [5.94300684e-03 9.94056940e-01]\n",
      " [9.98694122e-01 1.30594638e-03]\n",
      " [9.99996901e-01 3.13538271e-06]\n",
      " [1.30262405e-01 8.69737625e-01]\n",
      " [9.20314431e-01 7.96856210e-02]\n",
      " [1.12377457e-01 8.87622535e-01]\n",
      " [3.85320483e-04 9.99614596e-01]\n",
      " [1.34924157e-05 9.99986529e-01]\n",
      " [1.01447189e-02 9.89855289e-01]\n",
      " [9.99784768e-01 2.15190157e-04]\n",
      " [9.97246385e-01 2.75369151e-03]\n",
      " [9.98987854e-01 1.01219071e-03]\n",
      " [2.30843807e-06 9.99997735e-01]\n",
      " [9.55714703e-01 4.42853086e-02]\n",
      " [4.72083151e-01 5.27916849e-01]\n",
      " [9.99909282e-01 9.06570422e-05]\n",
      " [1.27818972e-01 8.72180998e-01]\n",
      " [1.18731618e-01 8.81268382e-01]\n",
      " [2.71417707e-01 7.28582263e-01]\n",
      " [9.01236475e-01 9.87636000e-02]\n",
      " [9.92669106e-01 7.33090425e-03]\n",
      " [2.40852842e-05 9.99975920e-01]\n",
      " [9.99713600e-01 2.86367402e-04]\n",
      " [2.93830759e-04 9.99706089e-01]\n",
      " [9.99834180e-01 1.65838792e-04]\n",
      " [2.11673859e-03 9.97883260e-01]\n",
      " [5.50804615e-01 4.49195355e-01]\n",
      " [9.75955188e-01 2.40448061e-02]\n",
      " [9.71255898e-01 2.87441444e-02]\n",
      " [1.47351325e-01 8.52648616e-01]\n",
      " [9.99957919e-01 4.21099721e-05]\n",
      " [7.39522511e-03 9.92604733e-01]\n",
      " [9.99977827e-01 2.21693263e-05]\n",
      " [9.92355227e-01 7.64479209e-03]\n",
      " [7.33523557e-05 9.99926686e-01]\n",
      " [1.46530254e-03 9.98534679e-01]\n",
      " [4.66780774e-02 9.53321993e-01]\n",
      " [8.80853713e-01 1.19146228e-01]\n",
      " [1.06949243e-04 9.99893069e-01]\n",
      " [9.99980330e-01 1.97115023e-05]\n",
      " [9.17581916e-01 8.24181512e-02]\n",
      " [2.01257646e-01 7.98742354e-01]\n",
      " [9.99569714e-01 4.30279120e-04]\n",
      " [9.78844702e-01 2.11552680e-02]\n",
      " [9.75734174e-01 2.42658108e-02]\n",
      " [1.60252675e-02 9.83974755e-01]\n",
      " [9.53459024e-01 4.65410054e-02]\n",
      " [9.99950051e-01 4.99763046e-05]\n",
      " [2.19165673e-03 9.97808397e-01]\n",
      " [9.99991536e-01 8.42678492e-06]\n",
      " [9.99835730e-01 1.64245575e-04]\n",
      " [9.99985337e-01 1.46061502e-05]\n",
      " [7.58252859e-01 2.41747141e-01]\n",
      " [9.99982119e-01 1.78573428e-05]\n",
      " [3.59044492e-01 6.40955448e-01]\n",
      " [3.33723985e-02 9.66627598e-01]\n",
      " [5.67815304e-01 4.32184696e-01]\n",
      " [8.08653980e-03 9.91913438e-01]\n",
      " [2.38393586e-05 9.99976158e-01]\n",
      " [9.99897361e-01 1.02635786e-04]\n",
      " [9.34273481e-01 6.57265782e-02]\n",
      " [6.73343778e-01 3.26656252e-01]\n",
      " [1.05768070e-01 8.94231915e-01]\n",
      " [7.93466151e-01 2.06533849e-01]\n",
      " [9.95471478e-01 4.52858862e-03]\n",
      " [4.01113182e-01 5.98886788e-01]\n",
      " [7.01194108e-01 2.98805892e-01]\n",
      " [9.99914646e-01 8.53581441e-05]\n",
      " [3.44084084e-01 6.55915916e-01]\n",
      " [2.15826273e-01 7.84173787e-01]\n",
      " [9.68536198e-01 3.14637721e-02]\n",
      " [9.61190343e-01 3.88096459e-02]\n",
      " [9.20121312e-01 7.98787251e-02]\n",
      " [9.99884009e-01 1.15963674e-04]\n",
      " [4.59438339e-02 9.54056084e-01]\n",
      " [1.63817815e-02 9.83618200e-01]\n",
      " [4.20119381e-04 9.99579847e-01]\n",
      " [4.23606515e-01 5.76393485e-01]\n",
      " [2.35555787e-02 9.76444483e-01]\n",
      " [9.99513984e-01 4.86059405e-04]\n",
      " [9.99900103e-01 9.98495379e-05]\n",
      " [9.99998093e-01 1.95644702e-06]\n",
      " [9.42968249e-01 5.70317134e-02]\n",
      " [8.31176996e-01 1.68822989e-01]\n",
      " [9.64358807e-01 3.56411859e-02]\n",
      " [9.91440773e-01 8.55925772e-03]\n",
      " [8.34524751e-01 1.65475205e-01]\n",
      " [9.99956012e-01 4.40150761e-05]\n",
      " [9.99990821e-01 9.15317742e-06]\n",
      " [9.48469155e-03 9.90515351e-01]\n",
      " [4.98050928e-01 5.01949072e-01]\n",
      " [9.71885264e-01 2.81147808e-02]\n",
      " [1.40490666e-01 8.59509289e-01]\n",
      " [7.94633175e-04 9.99205410e-01]\n",
      " [6.15191340e-01 3.84808570e-01]\n",
      " [5.53109467e-01 4.46890533e-01]\n",
      " [9.98830378e-01 1.16962276e-03]\n",
      " [3.51081090e-03 9.96489167e-01]\n",
      " [9.28405941e-01 7.15940893e-02]\n",
      " [9.55688238e-01 4.43117954e-02]\n",
      " [3.57474247e-03 9.96425211e-01]\n",
      " [9.99788105e-01 2.11947598e-04]\n",
      " [1.30367994e-01 8.69632006e-01]\n",
      " [3.36026475e-02 9.66397345e-01]\n",
      " [9.99986053e-01 1.39106314e-05]\n",
      " [9.98623013e-01 1.37697568e-03]\n",
      " [9.99951720e-01 4.82875876e-05]\n",
      " [9.85654771e-01 1.43452333e-02]\n",
      " [3.96119684e-01 6.03880346e-01]\n",
      " [8.62624962e-03 9.91373718e-01]\n",
      " [2.74237655e-02 9.72576201e-01]\n",
      " [9.93541718e-01 6.45824615e-03]\n",
      " [4.92900938e-01 5.07099092e-01]\n",
      " [1.98761169e-02 9.80123878e-01]\n",
      " [9.28535819e-01 7.14641511e-02]\n",
      " [6.91838622e-01 3.08161467e-01]\n",
      " [9.99992013e-01 8.02512477e-06]\n",
      " [9.74062085e-01 2.59378683e-02]\n",
      " [9.99975681e-01 2.43371160e-05]\n",
      " [3.53688345e-04 9.99646306e-01]\n",
      " [9.62131739e-01 3.78683433e-02]\n",
      " [9.99483824e-01 5.16110624e-04]\n",
      " [4.08042176e-03 9.95919585e-01]\n",
      " [3.06548690e-03 9.96934533e-01]\n",
      " [9.38060820e-01 6.19392209e-02]]\n",
      "[1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0\n",
      " 0 0 1 0 1 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 0 1 0 1\n",
      " 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0\n",
      " 0 1 0 0 1 0 0 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 1\n",
      " 0 0 1 0 1 1 0 0 0 0 1 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "X_test = text_to_index(test_text)\n",
    "X_test = pad_sequences(X_test, maxlen=PADDING_LENGTH)\n",
    "Y_preds = lstm.predict(X_test)\n",
    "print(Y_preds)\n",
    "Y_preds_label = np.argmax(Y_preds, axis=1)\n",
    "print(Y_preds_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67        99\n",
      "           1       0.57      0.57      0.57        76\n",
      "\n",
      "    accuracy                           0.62       175\n",
      "   macro avg       0.62      0.62      0.62       175\n",
      "weighted avg       0.62      0.62      0.62       175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#看評價\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, Y_preds_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 匯入所有的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_data = pd.read_csv(\"C:/Users/DA01001/Desktop/tebproject/0205NLP/no_cut_recovery_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = all_data[\"recovery\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25470"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 預測他"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-0467c45ce11a>:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(new_corpus)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.7349685e-01 2.6503149e-02]\n",
      " [9.9952352e-01 4.7644525e-04]\n",
      " [2.6068450e-03 9.9739313e-01]\n",
      " ...\n",
      " [9.9753040e-01 2.4696779e-03]\n",
      " [2.3602636e-01 7.6397365e-01]\n",
      " [4.0716209e-04 9.9959284e-01]]\n",
      "[0 0 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "X_test = text_to_index(all_text)\n",
    "X_test = pad_sequences(X_test, maxlen=PADDING_LENGTH)\n",
    "Y_preds = lstm.predict(X_test)\n",
    "print(Y_preds)\n",
    "Y_preds_label = np.argmax(Y_preds, axis=1)\n",
    "print(Y_preds_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25470"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_preds_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_preds_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把剛剛預測的資料加進原始資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"Y_preds_label\"] = Y_preds_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv('./savenocut/preds_label.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### group articleID 計算正向有幾個"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_total = all_data.groupby(\"articleID\")[\"Y_preds_label\"].count()\n",
    "group_positive = all_data.groupby(\"articleID\")[\"Y_preds_label\"].sum()\n",
    "#合併上面兩個項目(上面是serise)\n",
    "group = pd.DataFrame({'total':group_total, 'positive_total':group_positive})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把articleID從index取出來\n",
    "group['articleID'] = group.index\n",
    "group = group.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#負面的有幾個\n",
    "group[\"negative_total\"] = group[\"total\"] - group[\"positive_total\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合併原始資料和剛剛的計算資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_all_data = pd.merge(group,\n",
    "                          all_data[['articleID', 'url', 'title', 'createTime']],\n",
    "                          on='articleID',how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#刪除重複的資料+重製index\n",
    "count_all_data = count_all_data.drop_duplicates(subset='articleID', keep='first', inplace=False)\n",
    "count_all_data = count_all_data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算趴數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#趴數\n",
    "count_all_data[\"positive_percent\"] = count_all_data[\"positive_total\"] / count_all_data[\"total\"]\n",
    "count_all_data[\"negative_percent\"] = count_all_data[\"negative_total\"] / count_all_data[\"total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把他排的整齊一點"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_all_data = count_all_data[['articleID', 'url', 'title', 'createTime', 'positive_total', 'negative_total', 'total', 'positive_percent', 'negative_percent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_all_data.to_csv('./savenocut/count.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfive = count_all_data[count_all_data[\"total\"] > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1570"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 取出前3大的欄位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取出前3大的欄位\n",
    "positive_top = overfive.nlargest(3,'positive_percent')\n",
    "negative_top = overfive.nlargest(3,'negative_percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.875, 0.8421052631578947]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_top[\"url\"].tolist()\n",
    "positive_top[\"positive_percent\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 負向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_top[\"url\"].tolist()\n",
    "negative_top[\"negative_percent\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 篩選"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#篩選他\n",
    "def create_select_data(data):\n",
    "    select = input()\n",
    "    select_id = []\n",
    "    for i in range(0, len(data)):\n",
    "        if select in data[\"title\"][i] :\n",
    "            select_id.append(i)\n",
    "            #print(i, data[\"title\"][i])\n",
    "    select_data = data.loc[select_id]\n",
    "    select_data = select_data.reset_index(drop = True)\n",
    "    \n",
    "    return select_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#幫data加上一個預測完的欄位\n",
    "def preds_data(data):\n",
    "    #變成 list\n",
    "    all_text = data[\"recovery\"].tolist()\n",
    "    \n",
    "    #預測他\n",
    "    X_test = text_to_index(all_text)\n",
    "    X_test = pad_sequences(X_test, maxlen=PADDING_LENGTH)\n",
    "    Y_preds = lstm.predict(X_test)\n",
    "    Y_preds_label = np.argmax(Y_preds, axis=1)\n",
    "    \n",
    "    data[\"Y_preds_label\"] = Y_preds_label\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#產生 group他計算正負項\n",
    "#這裡的data必須要有Y_preds_label欄位\n",
    "\n",
    "def create_count_positivedata_negativedata(data):\n",
    "    group_total = data.groupby(\"articleID\")[\"Y_preds_label\"].count()\n",
    "    group_positive = data.groupby(\"articleID\")[\"Y_preds_label\"].sum()\n",
    "    #合併上面兩個項目(上面是serise)\n",
    "    group = pd.DataFrame({'total':group_total, 'positive_total':group_positive})\n",
    "    \n",
    "    group['articleID'] = group.index\n",
    "    group = group.reset_index(drop = True)\n",
    "    \n",
    "    #負面的有幾個\n",
    "    group[\"negative_total\"] = group[\"total\"] - group[\"positive_total\"]\n",
    "    \n",
    "    count_all_data = pd.merge(group,\n",
    "                          data[['articleID', 'url', 'title', 'createTime']],\n",
    "                          on='articleID',how = 'left')\n",
    "    \n",
    "    #刪除重複的資料+重製index\n",
    "    count_all_data = count_all_data.drop_duplicates(subset='articleID', keep='first', inplace=False)\n",
    "    count_all_data = count_all_data.reset_index(drop = True)\n",
    "    \n",
    "    #出現計算趴數欄位\n",
    "    count_all_data[\"positive_percent\"] = count_all_data[\"positive_total\"] / count_all_data[\"total\"]\n",
    "    count_all_data[\"negative_percent\"] = count_all_data[\"negative_total\"] / count_all_data[\"total\"]\n",
    "    \n",
    "    #排整齊\n",
    "    count_all_data = count_all_data[['articleID', 'url', 'title', 'createTime', 'positive_total', 'negative_total', 'total', 'positive_percent', 'negative_percent']]\n",
    "    overfive = count_all_data[count_all_data[\"total\"] > 3]\n",
    "    \n",
    "    return overfive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#產出正向前三名的兩個list\n",
    "\n",
    "def top3_positivedata(data):\n",
    "    positive_top = data.nlargest(3,'positive_percent')\n",
    "    \n",
    "    positive_url = positive_top[\"url\"].tolist()\n",
    "    positive_percent = positive_top[\"positive_percent\"].tolist()\n",
    "    \n",
    "    return positive_url, positive_percent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#產出負向前三名的兩個list\n",
    "\n",
    "def top3_negativedata(data):\n",
    "    negative_top = data.nlargest(3,'negative_percent')\n",
    "    \n",
    "    negative_url = negative_top[\"url\"].tolist()\n",
    "    negative_percent = negative_top[\"negative_percent\"].tolist()\n",
    "    \n",
    "    return negative_url, negative_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_data = create_select_data(all_data)\n",
    "preds = preds_data(select_data)\n",
    "count = create_count_positivedata_negativedata(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_positive_url = top3_positivedata(count)[0]\n",
    "top3_positive_percent = top3_positivedata(count)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_negative_url = top3_negativedata(count)[0]\n",
    "top3_negative_percent = top3_negativedata(count)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 合併"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一步到底的感覺\n",
    "def all_step(data):\n",
    "    select_data = create_select_data(data)\n",
    "    preds = preds_data(select_data)\n",
    "    count = create_count_positivedata_negativedata(preds)\n",
    "    top3_positive_url = top3_positivedata(count)[0]\n",
    "    top3_positive_percent = top3_positivedata(count)[1]\n",
    "    top3_negative_url = top3_negativedata(count)[0]\n",
    "    top3_negative_percent = top3_negativedata(count)[1]\n",
    "    \n",
    "    return top3_positive_url, top3_positive_percent, top3_negative_url, top3_negative_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "信義\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-0467c45ce11a>:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(new_corpus)\n"
     ]
    }
   ],
   "source": [
    "output = all_step(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive url list： ['https://www.myhousing.com.tw//index.php?option=com_kunena&view=topic&catid=8&id=100493&Itemid=0', 'https://www.mobile01.com/topicdetail.php?f=454&t=6019348', 'https://www.mobile01.com/topicdetail.php?f=454&t=6030435']\n",
      "positive percent list： [1.0, 0.6666666666666666, 0.5229357798165137]\n",
      "========================\n",
      "negative url list： ['https://www.mobile01.com/topicdetail.php?f=454&t=6056969', 'https://www.mobile01.com/topicdetail.php?f=454&t=5943880', 'https://www.mobile01.com/topicdetail.php?f=454&t=5911768']\n",
      "negative percent list： [0.8, 0.7727272727272727, 0.75]\n"
     ]
    }
   ],
   "source": [
    "print(\"positive url list：\" ,output[0])\n",
    "print(\"positive percent list：\" ,output[1])\n",
    "\n",
    "print(\"========================\")\n",
    "\n",
    "print(\"negative url list：\" ,output[2])\n",
    "print(\"negative percent list：\" ,output[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
