{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "\n",
    "model = Word2Vec.load(\"./model_Word2Vec\")\n",
    "all_data = pd.read_csv(\"./no_cut_recovery_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型設置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "embedding_matrix = np.zeros((len(model.wv.vocab.items()) + 1, model.vector_size))\n",
    "word2idx = {}\n",
    "PADDING_LENGTH = 500\n",
    "vocab_list = [(word, model.wv[word]) for word, _ in model.wv.vocab.items()]\n",
    "\n",
    "for i, vocab in enumerate(vocab_list):\n",
    "    word, vec = vocab\n",
    "    embedding_matrix[i + 1] = vec\n",
    "    word2idx[word] = i + 1\n",
    "\n",
    "embedding_layer = Embedding(input_dim=embedding_matrix.shape[0],\n",
    "                            output_dim=embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)\n",
    "\n",
    "#### mapping to index\n",
    "def text_to_index(corpus):\n",
    "    new_corpus = []\n",
    "    for doc in corpus:\n",
    "        new_doc = []\n",
    "        for word in doc:\n",
    "            try:\n",
    "                new_doc.append(word2idx[word])\n",
    "            except:\n",
    "                new_doc.append(0)\n",
    "        new_corpus.append(new_doc)\n",
    "    return np.array(new_corpus)\n",
    "\n",
    "def new_model():\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 匯入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Activation, LSTM\n",
    "import tensorflow as tf\n",
    "# load model\n",
    "json_file = open('./lstm.json', 'r')\n",
    "lstm_loaded_model_json = json_file.read()\n",
    "lstm = tf.keras.models.model_from_json(lstm_loaded_model_json)\n",
    "lstm.load_weights('./lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立篩選/計算結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#篩選他\n",
    "def create_select_data(data):\n",
    "    select = input()\n",
    "    select_id = []\n",
    "    for i in range(0, len(data)):\n",
    "        if select in data[\"title\"][i] :\n",
    "            select_id.append(i)\n",
    "            #print(i, data[\"title\"][i])\n",
    "    select_data = data.loc[select_id]\n",
    "    select_data = select_data.reset_index(drop = True)\n",
    "    \n",
    "    return select_data\n",
    "\n",
    "#幫data加上一個預測完的欄位\n",
    "def preds_data(data):\n",
    "    #變成 list\n",
    "    all_text = data[\"recovery\"].tolist()\n",
    "    \n",
    "    #預測他\n",
    "    X_test = text_to_index(all_text)\n",
    "    X_test = pad_sequences(X_test, maxlen=PADDING_LENGTH)\n",
    "    Y_preds = lstm.predict(X_test)\n",
    "    Y_preds_label = np.argmax(Y_preds, axis=1)\n",
    "    \n",
    "    data[\"Y_preds_label\"] = Y_preds_label\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "#產生 group他計算正負項\n",
    "#這裡的data必須要有Y_preds_label欄位\n",
    "\n",
    "def create_count_positivedata_negativedata(data):\n",
    "    group_total = data.groupby(\"articleID\")[\"Y_preds_label\"].count()\n",
    "    group_positive = data.groupby(\"articleID\")[\"Y_preds_label\"].sum()\n",
    "    #合併上面兩個項目(上面是serise)\n",
    "    group = pd.DataFrame({'total':group_total, 'positive_total':group_positive})\n",
    "    \n",
    "    group['articleID'] = group.index\n",
    "    group = group.reset_index(drop = True)\n",
    "    \n",
    "    #負面的有幾個\n",
    "    group[\"negative_total\"] = group[\"total\"] - group[\"positive_total\"]\n",
    "    \n",
    "    count_all_data = pd.merge(group,\n",
    "                          data[['articleID', 'url', 'title', 'createTime']],\n",
    "                          on='articleID',how = 'left')\n",
    "    \n",
    "    #刪除重複的資料+重製index\n",
    "    count_all_data = count_all_data.drop_duplicates(subset='articleID', keep='first', inplace=False)\n",
    "    count_all_data = count_all_data.reset_index(drop = True)\n",
    "    \n",
    "    #出現計算趴數欄位\n",
    "    count_all_data[\"positive_percent\"] = count_all_data[\"positive_total\"] / count_all_data[\"total\"]\n",
    "    count_all_data[\"negative_percent\"] = count_all_data[\"negative_total\"] / count_all_data[\"total\"]\n",
    "    \n",
    "    #排整齊\n",
    "    count_all_data = count_all_data[['articleID', 'url', 'title', 'createTime', 'positive_total', 'negative_total', 'total', 'positive_percent', 'negative_percent']]\n",
    "    overfive = count_all_data[count_all_data[\"total\"] > 3]\n",
    "    \n",
    "    return overfive\n",
    "\n",
    "\n",
    "#產出正向前三名的兩個list\n",
    "\n",
    "def top3_positivedata(data):\n",
    "    positive_top = data.nlargest(3,'positive_percent')\n",
    "    \n",
    "    positive_url = positive_top[\"url\"].tolist()\n",
    "    positive_percent = positive_top[\"positive_percent\"].tolist()\n",
    "    \n",
    "    return positive_url, positive_percent\n",
    "\n",
    "\n",
    "#產出負向前三名的兩個list\n",
    "\n",
    "def top3_negativedata(data):\n",
    "    negative_top = data.nlargest(3,'negative_percent')\n",
    "    \n",
    "    negative_url = negative_top[\"url\"].tolist()\n",
    "    negative_percent = negative_top[\"negative_percent\"].tolist()\n",
    "    \n",
    "    return negative_url, negative_percent\n",
    "\n",
    "\n",
    "#一步到底的感覺\n",
    "def all_step(data):\n",
    "    select_data = create_select_data(data)\n",
    "    preds = preds_data(select_data)\n",
    "    count = create_count_positivedata_negativedata(preds)\n",
    "    top3_positive_url = top3_positivedata(count)[0]\n",
    "    top3_positive_percent = top3_positivedata(count)[1]\n",
    "    top3_negative_url = top3_negativedata(count)[0]\n",
    "    top3_negative_percent = top3_negativedata(count)[1]\n",
    "    \n",
    "    return top3_positive_url, top3_positive_percent, top3_negative_url, top3_negative_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用他"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "信義\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-0467c45ce11a>:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(new_corpus)\n"
     ]
    }
   ],
   "source": [
    "output = all_step(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 輸出看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive url list： ['https://www.myhousing.com.tw//index.php?option=com_kunena&view=topic&catid=8&id=100493&Itemid=0', 'https://www.mobile01.com/topicdetail.php?f=454&t=6019348', 'https://www.mobile01.com/topicdetail.php?f=454&t=6030435']\n",
      "positive percent list： [1.0, 0.6666666666666666, 0.5229357798165137]\n",
      "========================\n",
      "negative url list： ['https://www.mobile01.com/topicdetail.php?f=454&t=6056969', 'https://www.mobile01.com/topicdetail.php?f=454&t=5943880', 'https://www.mobile01.com/topicdetail.php?f=454&t=5911768']\n",
      "negative percent list： [0.8, 0.7727272727272727, 0.75]\n"
     ]
    }
   ],
   "source": [
    "print(\"positive url list：\" ,output[0])\n",
    "print(\"positive percent list：\" ,output[1])\n",
    "\n",
    "print(\"========================\")\n",
    "\n",
    "print(\"negative url list：\" ,output[2])\n",
    "print(\"negative percent list：\" ,output[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
